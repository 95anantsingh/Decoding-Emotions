{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/'\n",
    "jobs_dir = os.path.join(base_dir,'sbatch')\n",
    "logs_base_dir = os.path.join(base_dir,'logs')\n",
    "os.makedirs(logs_base_dir,exist_ok=True)\n",
    "\n",
    "batch_number = max([int(d.split('_')[-1]) for d in os.listdir(logs_base_dir)]+[1])\n",
    "logs_dir = os.path.join(logs_base_dir,f'batch_{batch_number}')\n",
    "os.makedirs(logs_dir,exist_ok=True)\n",
    "\n",
    "if len(os.listdir(logs_dir))!=0:\n",
    "    logs_dir = os.path.join(logs_base_dir,f'batch_{batch_number+1}')\n",
    "\n",
    "scripts_dir = os.path.join(base_dir,'scripts')\n",
    "\n",
    "os.makedirs(jobs_dir,exist_ok=True)\n",
    "os.makedirs(logs_dir,exist_ok=True)\n",
    "os.makedirs(scripts_dir,exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Main Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbatch_header = f\"#!/bin/bash\\n\\\n",
    "\\n\\\n",
    "#SBATCH --nodes=1               \\n\\\n",
    "#SBATCH --ntasks-per-node=1     \\n\\\n",
    "#SBATCH --gres=gpu:1            \\n\"\n",
    "\n",
    "# partition list - sinfo -s\n",
    "\n",
    "job_name_directive =  \"#SBATCH --job-name=Job\"\n",
    "output_file_directive = \"#SBATCH --output=\"+logs_dir+'/job'\n",
    "\n",
    "command_header = \"\\nmodule purge\\n\\\n",
    "source ~/.bashrc\\n\\\n",
    "conda activate NLP_Nightly\\n\\\n",
    "cd /home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/\\n\\n\"\n",
    "\n",
    "# Main Commmand\n",
    "command = \"python main.py -ll debug -em gpu_memory -nw 2 -cm DENSE -e 50 \"\n",
    "\n",
    "# RAM required in GB\n",
    "memory_required_per_task = 100\n",
    "job_params = dict(\n",
    "    time = {\n",
    "        'AESDD':'00:35:00','CaFE':'00:35:00',\n",
    "        'EmoDB':'00:35:00','EMOVO':'00:35:00',\n",
    "        'IEMOCAP':'06:30:00','RAVDESS':'00:35:00',\n",
    "        'ShEMO':'04:00:00'},       # Time per job\n",
    "    # mem = '128GB', )               # RAM required in GB\n",
    "    partition = 'a100_1,a100_2,rtx8000,v100')\n",
    "\n",
    "# for param,val in job_params.items():\n",
    "#     sbatch_header+=f'#SBATCH --{param}={val}\\n'\n",
    "sbatch_header+=f'#SBATCH --partition={job_params[\"partition\"]}\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Run1','Run2','Run3','Run4','Run5']\n",
    "runs = ['Run1','Run2','Run3','Run4','Run5']\n",
    "# ['AESDD','CaFE','EmoDB','EMOVO','IEMOCAP','RAVDESS','ShEMO']\n",
    "datasets = ['AESDD','CaFE','EmoDB','EMOVO','IEMOCAP','RAVDESS','ShEMO']\n",
    "# ['GE2E','WAV2VEC2_BASE','WAV2VEC2_LARGE','WAV2VEC2_LARGE_XLSR','WAV2VEC2_LARGE_XLSR300M','HUBERT_BASE','HUBERT_LARGE']\n",
    "models = ['WAV2VEC2_LARGE_XLSR']\n",
    "\n",
    "# sbatch_header+=f'#SBATCH --cpus-per-task={len(runs)}\\n'\n",
    "# sbatch_header+=f'#SBATCH --mem={min(250,memory_required_per_task*len(runs))}GB\\n'\n",
    "\n",
    "sbatch_header+=f'#SBATCH --cpus-per-task=4\\n'\n",
    "sbatch_header+=f'#SBATCH --mem={memory_required_per_task}GB\\n'\n",
    "\n",
    "jobs = []\n",
    "times = []\n",
    "for run in runs:\n",
    "    for dataset in datasets:\n",
    "        for model in models:            \n",
    "            # c = f'{command} -d {dataset} -m {model} -r '\n",
    "            # for run in runs: c+=f' {run}'\n",
    "            # jobs.append(c)\n",
    "            times.append(f'#SBATCH --time={job_params[\"time\"][dataset]}\\n')\n",
    "            jobs.append(f'{command} -d {dataset} -fm {model} -r {run}')\n",
    "# print(sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]+1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make SBATCH Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_start_number = 1#max(1,sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]+1)\n",
    "\n",
    "# Number of jobs per GPU\n",
    "jobs_per_gpu = 1\n",
    "# Make sbatch files\n",
    "for i,j in enumerate(range(0,len(jobs),jobs_per_gpu),job_start_number):\n",
    "    with open(os.path.join(jobs_dir,'job'+str(i)+'.sbatch'),'w') as file:\n",
    "        file.write(sbatch_header+ times[j])\n",
    "        file.write(job_name_directive+str(i)+'\\n')\n",
    "        file.write(output_file_directive+str(i)+'.log\\n')\n",
    "        file.write(command_header)\n",
    "        for k in range(j,j+jobs_per_gpu):\n",
    "            jobs[k] += f' -jn Job{i}'\n",
    "            file.write(jobs[k])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Schedule File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedule_file = os.path.join(scripts_dir,'schedule_jobs.sh')\n",
    "# with open(schedule_file,'w') as file:\n",
    "#     file.write('#!/bin/bash\\n\\n')\n",
    "#     for k in range(job_start_number,len(jobs)+job_start_number):\n",
    "#         file.write('sbatch '+jobs_dir+'/job'+str(k)+'.sbatch\\n')\n",
    "# os.chmod(schedule_file, 0o740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]\n",
    "from_ = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[0]\n",
    "\n",
    "schedule_file = os.path.join(scripts_dir,'schedule_jobs.sh')\n",
    "with open(schedule_file,'w') as file:\n",
    "    file.write('#!/bin/bash\\n\\n')\n",
    "    for k in range(from_,to+1):\n",
    "        file.write('sbatch '+jobs_dir+'/job'+str(k)+'.sbatch\\n')\n",
    "os.chmod(schedule_file, 0o740)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Cancel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel_file = os.path.join(scripts_dir,'cancel_jobs.sh')\n",
    "# base_command = \"scancel $(sacct -n -X --format jobid --name\"\n",
    "# with open(cancel_file,'w') as file:\n",
    "#     file.write('#!/bin/bash\\n\\n')\n",
    "#     for k in range(job_start_number,len(jobs)+job_start_number):\n",
    "#         file.write(base_command+' Job'+str(k)+')\\n')\n",
    "# os.chmod(cancel_file, 0o740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]\n",
    "from_ = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[0]\n",
    "\n",
    "cancel_file = os.path.join(scripts_dir,'cancel_jobs.sh')\n",
    "base_command = \"scancel $(sacct -n -X --format jobid --name\"\n",
    "with open(cancel_file,'w') as file:\n",
    "    file.write('#!/bin/bash\\n\\n')\n",
    "    for k in range(from_,to+1):\n",
    "        file.write(base_command+' Job'+str(k)+')\\n')\n",
    "os.chmod(cancel_file, 0o740)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 30703551\n",
      "Submitted batch job 30703552\n",
      "Submitted batch job 30703553\n",
      "Submitted batch job 30703554\n",
      "Submitted batch job 30703555\n",
      "Submitted batch job 30703556\n",
      "Submitted batch job 30703557\n",
      "Submitted batch job 30703558\n",
      "Submitted batch job 30703559\n",
      "Submitted batch job 30703560\n",
      "Submitted batch job 30703561\n",
      "Submitted batch job 30703562\n",
      "Submitted batch job 30703563\n",
      "Submitted batch job 30703564\n",
      "Submitted batch job 30703565\n",
      "Submitted batch job 30703566\n",
      "Submitted batch job 30703567\n",
      "Submitted batch job 30703568\n",
      "Submitted batch job 30703569\n",
      "Submitted batch job 30703570\n",
      "Submitted batch job 30703571\n",
      "Submitted batch job 30703572\n",
      "Submitted batch job 30703573\n",
      "Submitted batch job 30703574\n",
      "Submitted batch job 30703575\n",
      "Submitted batch job 30703576\n",
      "Submitted batch job 30703577\n",
      "Submitted batch job 30703578\n",
      "Submitted batch job 30703579\n",
      "Submitted batch job 30703580\n",
      "Submitted batch job 30703581\n",
      "Submitted batch job 30703582\n",
      "Submitted batch job 30703583\n",
      "Submitted batch job 30703584\n",
      "Submitted batch job 30703585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/'\n",
    "scripts_dir = os.path.join(base_dir,'scripts')\n",
    "schedule_file = os.path.join(scripts_dir,'schedule_jobs.sh')\n",
    "# os.system(f'rm -rf {logs_dir}/*')\n",
    "os.system(f'bash {schedule_file}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/'\n",
    "scripts_dir = os.path.join(base_dir,'scripts')\n",
    "cancel_file = os.path.join(scripts_dir,'cancel_jobs.sh')\n",
    "os.system(f'bash {cancel_file}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Running Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('squeue -u $USER -t running')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pending Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          30703585 a100_1,a1    Job35  as14229 PD       0:00      1 (Priority)\n",
      "          30703584 a100_1,a1    Job34  as14229 PD       0:00      1 (Priority)\n",
      "          30703583 a100_1,a1    Job33  as14229 PD       0:00      1 (Priority)\n",
      "          30703582 a100_1,a1    Job32  as14229 PD       0:00      1 (Priority)\n",
      "          30703581 a100_1,a1    Job31  as14229 PD       0:00      1 (Priority)\n",
      "          30703580 a100_1,a1    Job30  as14229 PD       0:00      1 (Priority)\n",
      "          30703579 a100_1,a1    Job29  as14229 PD       0:00      1 (Priority)\n",
      "          30703578 a100_1,a1    Job28  as14229 PD       0:00      1 (Priority)\n",
      "          30703577 a100_1,a1    Job27  as14229 PD       0:00      1 (Priority)\n",
      "          30703576 a100_1,a1    Job26  as14229 PD       0:00      1 (Priority)\n",
      "          30703575 a100_1,a1    Job25  as14229 PD       0:00      1 (Priority)\n",
      "          30703574 a100_1,a1    Job24  as14229 PD       0:00      1 (Priority)\n",
      "          30703573 a100_1,a1    Job23  as14229 PD       0:00      1 (Priority)\n",
      "          30703572 a100_1,a1    Job22  as14229 PD       0:00      1 (Priority)\n",
      "          30703571 a100_1,a1    Job21  as14229 PD       0:00      1 (Priority)\n",
      "          30703570 a100_1,a1    Job20  as14229 PD       0:00      1 (Priority)\n",
      "          30703569 a100_1,a1    Job19  as14229 PD       0:00      1 (Priority)\n",
      "          30703568 a100_1,a1    Job18  as14229 PD       0:00      1 (Priority)\n",
      "          30703567 a100_1,a1    Job17  as14229 PD       0:00      1 (Priority)\n",
      "          30703566 a100_1,a1    Job16  as14229 PD       0:00      1 (Priority)\n",
      "          30703565 a100_1,a1    Job15  as14229 PD       0:00      1 (Priority)\n",
      "          30703564 a100_1,a1    Job14  as14229 PD       0:00      1 (Priority)\n",
      "          30703563 a100_1,a1    Job13  as14229 PD       0:00      1 (Priority)\n",
      "          30703562 a100_1,a1    Job12  as14229 PD       0:00      1 (Priority)\n",
      "          30703561 a100_1,a1    Job11  as14229 PD       0:00      1 (Priority)\n",
      "          30703560 a100_1,a1    Job10  as14229 PD       0:00      1 (Priority)\n",
      "          30703559 a100_1,a1     Job9  as14229 PD       0:00      1 (Priority)\n",
      "          30703558 a100_1,a1     Job8  as14229 PD       0:00      1 (Priority)\n",
      "          30703557 a100_1,a1     Job7  as14229 PD       0:00      1 (Priority)\n",
      "          30703556 a100_1,a1     Job6  as14229 PD       0:00      1 (Priority)\n",
      "          30703555 a100_1,a1     Job5  as14229 PD       0:00      1 (Priority)\n",
      "          30703554 a100_1,a1     Job4  as14229 PD       0:00      1 (Priority)\n",
      "          30703553 a100_1,a1     Job3  as14229 PD       0:00      1 (Priority)\n",
      "          30703552 a100_1,a1     Job2  as14229 PD       0:00      1 (Priority)\n",
      "          30703551 a100_1,a1     Job1  as14229 PD       0:00      1 (Priority)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('squeue -u $USER -t pending')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/sbatch'\n",
    "jobs = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "\n",
    "# for job in jobs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs:\n",
    "    with open(job, 'r') as file:\n",
    "        # read a list of lines into data\n",
    "        data = file.readlines()\n",
    "    data[6]='#SBATCH --partition=a100_1,a100_2,rtx8000,v100\\n'\n",
    "    with open(job, 'w') as file:\n",
    "        file.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_loc = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/sbatch/cancelled'\n",
    "for job in jobs:\n",
    "    with open(job, 'r') as file:\n",
    "        # read a list of lines into data\n",
    "        data = file.readlines()\n",
    "    if 'WAVLM' in data[-1] :\n",
    "        os.system(f'mv {job} {mv_loc}')\n",
    "\n",
    "# python main.py -ll debug -em gpu_memory -nw 2 -cm DENSE  -d IEMOCAP -m WAV2VEC2_BASE -r Run1 -jn Job1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb = sorted([int(m.split('.')[0][3:]) for m in os.listdir('/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/sbatch')])\n",
    "for j in jb :\n",
    "    os.system(f'scancel $(sacct -n -X --format jobid --name Job{j})')\n",
    "    # print(f'Job{j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb = sorted([int(m.split('.')[0][3:]) for m in os.listdir('/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/sbatch')])\n",
    "for j in jb :\n",
    "    os.system(f'sbatch /home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/sbatch/job{j}.sbatch')\n",
    "    # print(f'Job{j}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSERS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edf93a12d1b5ae7990448df3fb6c693d48c5ca93c427a6f205fa1a1fadc791bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
