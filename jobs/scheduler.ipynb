{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/'\n",
    "\n",
    "# Make base Log dir\n",
    "logs_base_dir = os.path.join(base_dir,'logs')\n",
    "os.makedirs(logs_base_dir,exist_ok=True)\n",
    "\n",
    "# Make logs dir for next batch\n",
    "batch_number = max([int(d.split('_')[-1]) for d in os.listdir(logs_base_dir)]+[1])\n",
    "logs_dir = os.path.join(logs_base_dir,f'batch_{batch_number}')\n",
    "os.makedirs(logs_dir,exist_ok=True)\n",
    "\n",
    "if len(os.listdir(logs_dir))!=0:\n",
    "    logs_dir = os.path.join(logs_base_dir,f'batch_{batch_number+1}')\n",
    "\n",
    "os.makedirs(logs_dir,exist_ok=True)\n",
    "\n",
    "# Make scripts dir\n",
    "scripts_dir = os.path.join(base_dir,'scripts')\n",
    "os.makedirs(scripts_dir,exist_ok=True)\n",
    "\n",
    "# Make jobs dir\n",
    "jobs_dir = os.path.join(base_dir,'sbatch')\n",
    "os.makedirs(jobs_dir,exist_ok=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbatch_header = f\"#!/bin/bash\\n\\\n",
    "\\n\\\n",
    "#SBATCH --nodes=1               \\n\\\n",
    "#SBATCH --ntasks-per-node=1     \\n\\\n",
    "#SBATCH --gres=gpu:1            \\n\"\n",
    "\n",
    "job_params = dict(\n",
    "    # time = { # for 50 epochs\n",
    "    #     'AESDD':'00:30:00','CaFE':'00:40:00',\n",
    "    #     'EmoDB':'00:30:00','EMOVO':'00:30:00',\n",
    "    #     'IEMOCAP':'08:30:00','RAVDESS':'00:40:00',\n",
    "    #     'ShEMO':'05:00:00'},      \n",
    "    time = { # for 30 epochs\n",
    "        'AESDD':'00:20:00','CaFE':'00:30:00',\n",
    "        'EmoDB':'00:20:00','EMOVO':'00:20:00',\n",
    "        'IEMOCAP':'05:30:00','RAVDESS':'00:30:00',\n",
    "        'ShEMO':'02:45:00'},                # Time per job\n",
    "    memory = '100GB',                       # RAM required in GB\n",
    "    partition = 'a100_1,a100_2,rtx8000')    # GPUs you want, to list all available run - partition list - sinfo -s\n",
    "\n",
    "sbatch_header+=f'#SBATCH --partition={job_params[\"partition\"]}\\n'\n",
    "sbatch_header+=f'#SBATCH --cpus-per-task=4\\n'\n",
    "sbatch_header+=f'#SBATCH --mem={job_params[\"memory\"]}GB\\n'\n",
    "\n",
    "job_name_directive =  \"#SBATCH --job-name=Job\"\n",
    "output_file_directive = \"#SBATCH --output=\"+logs_dir+'/job'\n",
    "\n",
    "# Command Header\n",
    "command_header = \"\\n\\\n",
    "source ~/.bashrc\\n\\\n",
    "conda activate MSERS\\n\\\n",
    "cd /home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/\\n\\n\"\n",
    "\n",
    "# Main Commmand\n",
    "command = \"python main.py -ll debug -em gpu_memory -nw 3 -cm CM_PROBING_LINEAR -e 30 \""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Full Commands and Walltimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Run1','Run2','Run3','Run4','Run5']\n",
    "runs = ['Run1','Run2']\n",
    "# ['AESDD','CaFE','EmoDB','EMOVO','IEMOCAP','RAVDESS','ShEMO']\n",
    "datasets = ['AESDD','IEMOCAP']\n",
    "# ['GE2E','WAV2VEC2_BASE','WAV2VEC2_LARGE','WAV2VEC2_LARGE_XLSR','WAV2VEC2_LARGE_XLSR300M','HUBERT_BASE','HUBERT_LARGE','WAV2VEC2_ASR_LARGE_960H','HUBERT_ASR_LARGE']\n",
    "models = ['WAV2VEC2_BASE','HUBERT_ASR_LARGE']\n",
    "\n",
    "jobs = []\n",
    "times = []\n",
    "for run in runs:\n",
    "    for dataset in datasets:\n",
    "        for model in models:            \n",
    "            times.append(f'#SBATCH --time={job_params[\"time\"][dataset]}\\n')\n",
    "            jobs.append(f'{command} -d {dataset} -fm {model} -r {run}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make SBATCH Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Next Job number\n",
    "try: job_start_number = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]+1\n",
    "except: job_start_number = 1\n",
    "\n",
    "# Number of consecutive jobs per GPU\n",
    "jobs_per_gpu = 1\n",
    "\n",
    "# Make sbatch files\n",
    "for i,j in enumerate(range(0,len(jobs),jobs_per_gpu),job_start_number):\n",
    "    with open(os.path.join(jobs_dir,'job'+str(i)+'.sbatch'),'w') as file:\n",
    "        file.write(sbatch_header+ times[j])\n",
    "        file.write(job_name_directive+str(i)+'\\n')\n",
    "        file.write(output_file_directive+str(i)+'.log\\n')\n",
    "        file.write(command_header)\n",
    "        for k in range(j,j+jobs_per_gpu):\n",
    "            jobs[k] += f' -jn Job{i}'\n",
    "            file.write(jobs[k])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Schedule File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]\n",
    "from_ = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[0]\n",
    "\n",
    "schedule_file = os.path.join(scripts_dir,'schedule_jobs.sh')\n",
    "with open(schedule_file,'w') as file:\n",
    "    file.write('#!/bin/bash\\n\\n')\n",
    "    for k in range(from_,to+1):\n",
    "        file.write('sbatch '+jobs_dir+'/job'+str(k)+'.sbatch\\n')\n",
    "os.chmod(schedule_file, 0o740)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Cancel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]\n",
    "from_ = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[0]\n",
    "\n",
    "cancel_file = os.path.join(scripts_dir,'cancel_jobs.sh')\n",
    "base_command = \"scancel $(sacct -n -X --format jobid --name\"\n",
    "with open(cancel_file,'w') as file:\n",
    "    file.write('#!/bin/bash\\n\\n')\n",
    "    for k in range(from_,to+1):\n",
    "        file.write(base_command+' Job'+str(k)+')\\n')\n",
    "os.chmod(cancel_file, 0o740)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/'\n",
    "scripts_dir = os.path.join(base_dir,'scripts')\n",
    "schedule_file = os.path.join(scripts_dir,'schedule_jobs.sh')\n",
    "# os.system(f'rm -rf {logs_dir}/*')\n",
    "os.system(f'bash {schedule_file}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/'\n",
    "scripts_dir = os.path.join(base_dir,'scripts')\n",
    "cancel_file = os.path.join(scripts_dir,'cancel_jobs.sh')\n",
    "os.system(f'bash {cancel_file}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Running Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('squeue -u $USER -t running')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pending Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('squeue -u $USER -t pending')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSERS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edf93a12d1b5ae7990448df3fb6c693d48c5ca93c427a6f205fa1a1fadc791bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
