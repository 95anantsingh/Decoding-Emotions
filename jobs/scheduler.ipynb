{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/'\n",
    "jobs_dir = os.path.join(base_dir,'sbatch')\n",
    "logs_base_dir = os.path.join(base_dir,'logs')\n",
    "os.makedirs(logs_base_dir,exist_ok=True)\n",
    "\n",
    "batch_number = max([int(d.split('_')[-1]) for d in os.listdir(logs_base_dir)]+[1])\n",
    "logs_dir = os.path.join(logs_base_dir,f'batch_{batch_number}')\n",
    "os.makedirs(logs_dir,exist_ok=True)\n",
    "\n",
    "if len(os.listdir(logs_dir))!=0:\n",
    "    logs_dir = os.path.join(logs_base_dir,f'batch_{batch_number+1}')\n",
    "\n",
    "scripts_dir = os.path.join(base_dir,'scripts')\n",
    "\n",
    "os.makedirs(jobs_dir,exist_ok=True)\n",
    "os.makedirs(logs_dir,exist_ok=True)\n",
    "os.makedirs(scripts_dir,exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Main Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbatch_header = f\"#!/bin/bash\\n\\\n",
    "\\n\\\n",
    "#SBATCH --nodes=1               \\n\\\n",
    "#SBATCH --ntasks-per-node=1     \\n\\\n",
    "#SBATCH --gres=gpu:1            \\n\"\n",
    "\n",
    "# partition list - sinfo -s\n",
    "\n",
    "job_name_directive =  \"#SBATCH --job-name=Job\"\n",
    "output_file_directive = \"#SBATCH --output=\"+logs_dir+'/job'\n",
    "\n",
    "command_header = \"\\n\\\n",
    "source ~/.bashrc\\n\\\n",
    "conda activate MSERS\\n\\\n",
    "cd /home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/\\n\\n\"\n",
    "\n",
    "# Main Commmand\n",
    "command = \"python main.py -ll debug -em gpu_memory -nw 3 -cm CM_PROBING_LINEAR -e 30 \"\n",
    "\n",
    "# RAM required in GB\n",
    "memory_required_per_task = 100\n",
    "\n",
    "job_params = dict(\n",
    "    time = {\n",
    "        'AESDD':'00:20:00','CaFE':'00:30:00',\n",
    "        'EmoDB':'00:20:00','EMOVO':'00:20:00',\n",
    "        'IEMOCAP':'05:30:00','RAVDESS':'00:30:00',\n",
    "        'ShEMO':'02:45:00'},       # Time per job\n",
    "    # mem = '128GB', )               # RAM required in GB\n",
    "    partition = 'a100_1,a100_2,rtx8000')\n",
    "\n",
    "# job_params = dict(\n",
    "#     time = {\n",
    "#         'AESDD':'00:30:00','CaFE':'00:40:00',\n",
    "#         'EmoDB':'00:30:00','EMOVO':'00:30:00',\n",
    "#         'IEMOCAP':'08:30:00','RAVDESS':'00:40:00',\n",
    "#         'ShEMO':'05:00:00'},       # Time per job\n",
    "#     # mem = '128GB', )               # RAM required in GB\n",
    "#     partition = 'a100_1,a100_2,rtx8000')\n",
    "\n",
    "# for param,val in job_params.items():\n",
    "#     sbatch_header+=f'#SBATCH --{param}={val}\\n'\n",
    "sbatch_header+=f'#SBATCH --partition={job_params[\"partition\"]}\\n'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IEMOCAP FX_WAV2VEC2_ASR_LARGE_960H_CLF_CM_PROBING_DENSE Run1\n",
    "IEMOCAP FX_WAV2VEC2_ASR_LARGE_960H_CLF_CM_PROBING_DENSE Run2\n",
    "\n",
    "IEMOCAP FX_HUBERT_ASR_LARGE_CLF_CM_PROBING_DENSE Run1\n",
    "IEMOCAP FX_HUBERT_ASR_LARGE_CLF_CM_PROBING_DENSE Run2\n",
    "\n",
    "<!-- IEMOCAP FX_HUBERT_ASR_LARGE_CLF_CM_PROBING_LINEAR Run2 -->\n",
    "\n",
    "IEMOCAP FX_HUBERT_LARGE_CLF_CM_PROBING_DENSE Run1\n",
    "IEMOCAP FX_HUBERT_LARGE_CLF_CM_PROBING_DENSE Run2\n",
    "\n",
    "IEMOCAP FX_WAV2VEC2_LARGE_CLF_CM_PROBING_DENSE Run1\n",
    "IEMOCAP FX_WAV2VEC2_LARGE_CLF_CM_PROBING_DENSE Run2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Run1','Run2','Run3','Run4','Run5']\n",
    "runs = ['Run2']\n",
    "# ['AESDD','CaFE','EmoDB','EMOVO','IEMOCAP','RAVDESS','ShEMO']\n",
    "datasets = ['IEMOCAP']\n",
    "# ['GE2E','WAV2VEC2_BASE','WAV2VEC2_LARGE','WAV2VEC2_LARGE_XLSR','WAV2VEC2_LARGE_XLSR300M','HUBERT_BASE','HUBERT_LARGE','WAV2VEC2_ASR_LARGE_960H','HUBERT_ASR_LARGE']\n",
    "models = ['HUBERT_ASR_LARGE']\n",
    "\n",
    "# sbatch_header+=f'#SBATCH --cpus-per-task={len(runs)}\\n'\n",
    "# sbatch_header+=f'#SBATCH --mem={min(250,memory_required_per_task*len(runs))}GB\\n'\n",
    "\n",
    "sbatch_header+=f'#SBATCH --cpus-per-task=4\\n'\n",
    "sbatch_header+=f'#SBATCH --mem={memory_required_per_task}GB\\n'\n",
    "\n",
    "jobs = []\n",
    "times = []\n",
    "for run in runs:\n",
    "    for dataset in datasets:\n",
    "        for model in models:            \n",
    "            # c = f'{command} -d {dataset} -m {model} -r '\n",
    "            # for run in runs: c+=f' {run}'\n",
    "            # jobs.append(c)\n",
    "            times.append(f'#SBATCH --time={job_params[\"time\"][dataset]}\\n')\n",
    "            jobs.append(f'{command} -d {dataset} -fm {model} -r {run}')\n",
    "# print(sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]+1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make SBATCH Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_start_number = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]+1\n",
    "# job_start_number = 1\n",
    "\n",
    "# Number of jobs per GPU\n",
    "jobs_per_gpu = 1\n",
    "# Make sbatch files\n",
    "for i,j in enumerate(range(0,len(jobs),jobs_per_gpu),job_start_number):\n",
    "    with open(os.path.join(jobs_dir,'job'+str(i)+'.sbatch'),'w') as file:\n",
    "        file.write(sbatch_header+ times[j])\n",
    "        file.write(job_name_directive+str(i)+'\\n')\n",
    "        file.write(output_file_directive+str(i)+'.log\\n')\n",
    "        file.write(command_header)\n",
    "        for k in range(j,j+jobs_per_gpu):\n",
    "            jobs[k] += f' -jn Job{i}'\n",
    "            file.write(jobs[k])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Schedule File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]\n",
    "from_ = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[0]\n",
    "\n",
    "schedule_file = os.path.join(scripts_dir,'schedule_jobs.sh')\n",
    "with open(schedule_file,'w') as file:\n",
    "    file.write('#!/bin/bash\\n\\n')\n",
    "    for k in range(from_,to+1):\n",
    "        file.write('sbatch '+jobs_dir+'/job'+str(k)+'.sbatch\\n')\n",
    "os.chmod(schedule_file, 0o740)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Cancel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[-1]\n",
    "from_ = sorted([int(m.split('.')[0][3:]) for m in os.listdir('sbatch')])[0]\n",
    "\n",
    "cancel_file = os.path.join(scripts_dir,'cancel_jobs.sh')\n",
    "base_command = \"scancel $(sacct -n -X --format jobid --name\"\n",
    "with open(cancel_file,'w') as file:\n",
    "    file.write('#!/bin/bash\\n\\n')\n",
    "    for k in range(from_,to+1):\n",
    "        file.write(base_command+' Job'+str(k)+')\\n')\n",
    "os.chmod(cancel_file, 0o740)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 31710832\n",
      "Submitted batch job 31710833\n",
      "Submitted batch job 31710834\n",
      "Submitted batch job 31710835\n",
      "Submitted batch job 31710836\n",
      "Submitted batch job 31710837\n",
      "Submitted batch job 31710838\n",
      "Submitted batch job 31710839\n",
      "Submitted batch job 31710840\n",
      "Submitted batch job 31710841\n",
      "Submitted batch job 31710842\n",
      "Submitted batch job 31710843\n",
      "Submitted batch job 31710844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/'\n",
    "scripts_dir = os.path.join(base_dir,'scripts')\n",
    "schedule_file = os.path.join(scripts_dir,'schedule_jobs.sh')\n",
    "# os.system(f'rm -rf {logs_dir}/*')\n",
    "os.system(f'bash {schedule_file}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/'\n",
    "scripts_dir = os.path.join(base_dir,'scripts')\n",
    "cancel_file = os.path.join(scripts_dir,'cancel_jobs.sh')\n",
    "os.system(f'bash {cancel_file}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Running Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('squeue -u $USER -t running')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pending Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          31710844 a100_1,a1    Job13  as14229 PD       0:00      1 (Priority)\n",
      "          31710843 a100_1,a1    Job12  as14229 PD       0:00      1 (Priority)\n",
      "          31710842 a100_1,a1    Job11  as14229 PD       0:00      1 (Priority)\n",
      "          31710841 a100_1,a1    Job10  as14229 PD       0:00      1 (Priority)\n",
      "          31710840 a100_1,a1     Job9  as14229 PD       0:00      1 (Priority)\n",
      "          31710839 a100_1,a1     Job8  as14229 PD       0:00      1 (Priority)\n",
      "          31710838 a100_1,a1     Job7  as14229 PD       0:00      1 (Priority)\n",
      "          31710837 a100_1,a1     Job6  as14229 PD       0:00      1 (Priority)\n",
      "          31710836 a100_1,a1     Job5  as14229 PD       0:00      1 (Priority)\n",
      "          31710835 a100_1,a1     Job4  as14229 PD       0:00      1 (Priority)\n",
      "          31710834 a100_1,a1     Job3  as14229 PD       0:00      1 (Priority)\n",
      "          31710833 a100_1,a1     Job2  as14229 PD       0:00      1 (Priority)\n",
      "          31710832 a100_1,a1     Job1  as14229 PD       0:00      1 (Priority)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('squeue -u $USER -t pending')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/sbatch'\n",
    "jobs = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "\n",
    "# for job in jobs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs:\n",
    "    with open(job, 'r') as file:\n",
    "        # read a list of lines into data\n",
    "        data = file.readlines()\n",
    "    data[6]='#SBATCH --partition=a100_1,a100_2,rtx8000,v100\\n'\n",
    "    with open(job, 'w') as file:\n",
    "        file.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_loc = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/sbatch/cancelled'\n",
    "for job in jobs:\n",
    "    with open(job, 'r') as file:\n",
    "        # read a list of lines into data\n",
    "        data = file.readlines()\n",
    "    if 'WAVLM' in data[-1] :\n",
    "        os.system(f'mv {job} {mv_loc}')\n",
    "\n",
    "# python main.py -ll debug -em gpu_memory -nw 2 -cm DENSE  -d IEMOCAP -m WAV2VEC2_BASE -r Run1 -jn Job1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb = sorted([int(m.split('.')[0][3:]) for m in os.listdir('/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/cancelled')])\n",
    "for j in jb :\n",
    "    os.system(f'scancel $(sacct -n -X --format jobid --name Job{j})')\n",
    "    # print(f'Job{j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 30919018\n",
      "Submitted batch job 30919019\n",
      "Submitted batch job 30919020\n",
      "Submitted batch job 30919021\n",
      "Submitted batch job 30919022\n",
      "Submitted batch job 30919023\n",
      "Submitted batch job 30919024\n",
      "Submitted batch job 30919025\n",
      "Submitted batch job 30919026\n",
      "Submitted batch job 30919027\n",
      "Submitted batch job 30919028\n",
      "Submitted batch job 30919029\n",
      "Submitted batch job 30919030\n",
      "Submitted batch job 30919031\n",
      "Submitted batch job 30919032\n",
      "Submitted batch job 30919033\n",
      "Submitted batch job 30919034\n",
      "Submitted batch job 30919035\n",
      "Submitted batch job 30919036\n",
      "Submitted batch job 30919037\n",
      "Submitted batch job 30919038\n",
      "Submitted batch job 30919039\n",
      "Submitted batch job 30919040\n",
      "Submitted batch job 30919041\n",
      "Submitted batch job 30919042\n",
      "Submitted batch job 30919043\n",
      "Submitted batch job 30919044\n",
      "Submitted batch job 30919045\n",
      "Submitted batch job 30919046\n",
      "Submitted batch job 30919047\n",
      "Submitted batch job 30919048\n",
      "Submitted batch job 30919049\n",
      "Submitted batch job 30919050\n",
      "Submitted batch job 30919051\n",
      "Submitted batch job 30919052\n",
      "Submitted batch job 30919053\n",
      "Submitted batch job 30919054\n",
      "Submitted batch job 30919055\n",
      "Submitted batch job 30919056\n",
      "Submitted batch job 30919057\n",
      "Submitted batch job 30919058\n",
      "Submitted batch job 30919059\n",
      "Submitted batch job 30919060\n",
      "Submitted batch job 30919061\n",
      "Submitted batch job 30919062\n",
      "Submitted batch job 30919063\n",
      "Submitted batch job 30919064\n",
      "Submitted batch job 30919065\n",
      "Submitted batch job 30919066\n",
      "Submitted batch job 30919067\n",
      "Submitted batch job 30919068\n",
      "Submitted batch job 30919069\n",
      "Submitted batch job 30919070\n",
      "Submitted batch job 30919071\n",
      "Submitted batch job 30919072\n",
      "Submitted batch job 30919073\n",
      "Submitted batch job 30919074\n",
      "Submitted batch job 30919075\n",
      "Submitted batch job 30919076\n",
      "Submitted batch job 30919077\n",
      "Submitted batch job 30919078\n",
      "Submitted batch job 30919079\n",
      "Submitted batch job 30919080\n",
      "Submitted batch job 30919081\n",
      "Submitted batch job 30919082\n",
      "Submitted batch job 30919083\n",
      "Submitted batch job 30919084\n",
      "Submitted batch job 30919085\n",
      "Submitted batch job 30919086\n",
      "Submitted batch job 30919087\n"
     ]
    }
   ],
   "source": [
    "jb = sorted([int(m.split('.')[0][3:]) for m in os.listdir('/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/cancelled')])\n",
    "for j in jb :\n",
    "    os.system(f'sbatch /home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/cancelled/job{j}.sbatch')\n",
    "    # print(f'Job{j}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d RAVDESS -fm WAV2VEC2_LARGE_XLSR -r Run2 -jn Job188\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d CaFE -fm WAV2VEC2_LARGE_XLSR -r Run3 -jn Job191\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d IEMOCAP -fm WAV2VEC2_LARGE_XLSR -r Run3 -jn Job194\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d RAVDESS -fm WAV2VEC2_LARGE_XLSR -r Run4 -jn Job202\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d RAVDESS -fm WAV2VEC2_LARGE_XLSR -r Run1 -jn Job181\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d EMOVO -fm WAV2VEC2_LARGE_XLSR -r Run5 -jn Job207\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d CaFE -fm WAV2VEC2_LARGE_XLSR -r Run2 -jn Job184\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d ShEMO -fm WAV2VEC2_LARGE_XLSR -r Run1 -jn Job182\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d CaFE -fm WAV2VEC2_LARGE_XLSR -r Run5 -jn Job205\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d ShEMO -fm WAV2VEC2_LARGE_XLSR -r Run2 -jn Job189\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d EmoDB -fm WAV2VEC2_LARGE_XLSR -r Run3 -jn Job192\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d RAVDESS -fm WAV2VEC2_LARGE_XLSR -r Run5 -jn Job209\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d ShEMO -fm WAV2VEC2_LARGE_XLSR -r Run3 -jn Job196\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d AESDD -fm WAV2VEC2_LARGE_XLSR -r Run1 -jn Job176\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d EMOVO -fm WAV2VEC2_LARGE_XLSR -r Run1 -jn Job179\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d AESDD -fm WAV2VEC2_LARGE_XLSR -r Run4 -jn Job197\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d AESDD -fm WAV2VEC2_LARGE_XLSR -r Run5 -jn Job204\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d EmoDB -fm WAV2VEC2_LARGE_XLSR -r Run4 -jn Job199\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d AESDD -fm WAV2VEC2_LARGE_XLSR -r Run3 -jn Job190\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d ShEMO -fm WAV2VEC2_LARGE_XLSR -r Run4 -jn Job203\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d CaFE -fm WAV2VEC2_LARGE_XLSR -r Run1 -jn Job177\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d AESDD -fm WAV2VEC2_LARGE_XLSR -r Run2 -jn Job183\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d EmoDB -fm WAV2VEC2_LARGE_XLSR -r Run5 -jn Job206\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d IEMOCAP -fm WAV2VEC2_LARGE_XLSR -r Run4 -jn Job201\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d CaFE -fm WAV2VEC2_LARGE_XLSR -r Run4 -jn Job198\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d IEMOCAP -fm WAV2VEC2_LARGE_XLSR -r Run5 -jn Job208\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d ShEMO -fm WAV2VEC2_LARGE_XLSR -r Run5 -jn Job210\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d IEMOCAP -fm WAV2VEC2_LARGE_XLSR -r Run2 -jn Job187\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d EMOVO -fm WAV2VEC2_LARGE_XLSR -r Run3 -jn Job193\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d EmoDB -fm WAV2VEC2_LARGE_XLSR -r Run2 -jn Job185\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d EMOVO -fm WAV2VEC2_LARGE_XLSR -r Run2 -jn Job186\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d EMOVO -fm WAV2VEC2_LARGE_XLSR -r Run4 -jn Job200\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d RAVDESS -fm WAV2VEC2_LARGE_XLSR -r Run3 -jn Job195\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d EmoDB -fm WAV2VEC2_LARGE_XLSR -r Run1 -jn Job178\n",
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DENSE -e 50 -d IEMOCAP -fm WAV2VEC2_LARGE_XLSR -r Run1 -jn Job180\n"
     ]
    }
   ],
   "source": [
    "mv_loc = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/cancelled'\n",
    "job_dir = '/home/as14229/NYU_HPC/Multilingual-Speech-Emotion-Recognition-System/jobs/sbatch'\n",
    "for job in os.listdir(mv_loc):\n",
    "    jobfile = os.path.join(mv_loc,job)\n",
    "    with open(jobfile, 'r') as file:\n",
    "        # read a list of lines into data\n",
    "        data = file.readlines()\n",
    "    if 'PROBING_DEN50' in data[-1] :\n",
    "        data[-1] = data[-1][:61]+'SE'+data[-1][63:67]+'50'+data[-1][70:]\n",
    "        print(data[-1])\n",
    "        with open(jobfile, 'w') as file:\n",
    "            file.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30 '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING_DEN50 -e 30  -d CaFE -fm WAV2VEC2_LARGE_XLSR -r Run3 -jn Job191'\n",
    "a[67:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING -e 50  -d EmoDB -fm WAV2VEC2_LARGE_XLSR -r Run2 -jn Job220\n"
     ]
    }
   ],
   "source": [
    "a = 'python main.py -ll debug -em gpu_memory -nw 3 -cm PROBING -e 30  -d EmoDB -fm WAV2VEC2_LARGE_XLSR -r Run2 -jn Job220'\n",
    "a=a[:61]+'50'+a[63:]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSERS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edf93a12d1b5ae7990448df3fb6c693d48c5ca93c427a6f205fa1a1fadc791bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
